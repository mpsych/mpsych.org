%
% 2025
%
@INPROCEEDINGS{fernandez2025ctrlempower,
  author={Fernandez, Arianna and de Oliveira, Sophia and Spencer, Chaya and Hernandez, Kalysha Melendez and Piam, Samatrai and \myname{Haehn, Daniel} and Potasznik, Amanda},
  booktitle={IEEE International Symposium on Ethics in Science, Technology and Engineering (ETHICS)}, 
  title={CTRL + Empower: Lived Experiences of Black Women and Latinas in
Computer Science at an Urban Public University}, 
  abstract={Among public universities in the New England
region of the United States, the University of Massachusetts
Boston (UMB) is the most ethnically diverse. Enrollment data in
the Computer Science (CS) department there closely aligns with
the broader demographic composition of the university, with over
half of CS majors identifying as students of color, a trend that
deviates from national patterns. However, the department
demographics exhibit a gender disparity, with male students
outnumbering female students at a ratio of five to one, consistent
with both national and global trends in CS. The disparity is even
higher when an intersectional lens is applied: attracting and
retaining Black and Indigenous Women of Color (BIWOC)
students, especially Black and Latina women, in the CS program
remains a significant challenge. Here, an exploratory
phenomenological case study utilizing participatory action
research was conducted to examine the lived experiences of female
Black and Latina students in the CS department. A preliminary
survey informed focus group topics of discussion. Thematic coding
of focus group discussions was undertaken to achieve two
objectives: (1) to develop actionable recommendations for
improving the recruitment and retention of students of color and
women, and (2) establish research experience for the female
participants so they may benefit tangibly from their contribution.
Key themes identified included resistance to BIWOC students,
mostly in the form of questioning capabilities and other
microaggressions, background mental labor in responding to or
actively ignoring that resistance, navigating the world as it is
versus trying to change it for the better, the need for counterspaces to share experiences of microaggressions and develop
success strategies, and potential benefits of a BIWOC research
hub, open to all students but with a focus on BIWOC students.},
  year={2025},
  volume={},
  number={},
  pages={},
  doi={},    
  shortvenue={IEEE ETHICS 2025},
  entrysubtype = {fromUMB}
}

@article {nguyen2025evaluating,
    title={Evaluating 'Graphical Perception' with Multimodal LLMs},
  author={Nguyen, Rami Huu and Maeda, Kenichi and Geshvadi, Mahsa and \myname{Haehn, Daniel}},
    abstract={Multimodal Large Language Models (MLLMs) have remarkably progressed in analyzing and understanding images. Despite these advancements, accurately regressing values in charts remains an underexplored area for MLLMs. For visualization, how do MLLMs perform when applied to graphical perception tasks? Our paper investigates this question by reproducing Cleveland and McGill's seminal 1984 experiment and comparing it against human task performance. Our study primarily evaluates fine-tuned and pretrained models and zero-shot prompting to determine if they closely match human graphical perception. Our findings highlight that MLLMs outperform human task performance in some cases but not in others. We highlight the results of all experiments to foster an understanding of where MLLMs succeed and fail when applied to data visualization.},
    journal={IEEE Pacific Visualization (PacificVis)},
    year={2025},
    code={https://github.com/raminguyen/LLMP2},
    data={https://github.com/raminguyen/LLMP2},
    supplemental={https://mpsych.org/papers/nguyen2025_supplemental.pdf},
    shortvenue={PacificVis 2025}
}


@article{kim2025melanoma,
title={Melanoma Detection with Uncertainty Quantification},
  author={Kim, SangHyuk and Gaibor, Edward and Matejek, Brian and \myname{Haehn, Daniel}},
  abstract={  Early detection of melanoma is crucial for improving survival rates. Current detection tools often utilize data-driven machine learning methods but often overlook the full integration of multiple datasets. We combine publicly available datasets to enhance data diversity, allowing numerous experiments to train and evaluate various classifiers. We then calibrate them to minimize misdiagnoses by incorporating uncertainty quantification. Our experiments on benchmark datasets show accuracies of up to 93.2% before and 97.8% after applying uncertainty-based rejection, leading to a reduction in misdiagnoses by over 40.5%. Our code and data are publicly available, and a web-based interface for quick melanoma detection of user-supplied images is also provided.},
  booktitle={IEEE International Symposium on Biomedical Imaging (ISBI)},
  pages={TBA},
  year={2025},
  organization={IEEE},
  code={https://mpsych.org/melanoma/},
  data={https://mpsych.org/melanoma/},
  website={https://mpsych.github.io/melanoma/},
  shortvenue={ISBI 2025}
}

@article{gaibor2025boostlet,
title={Boostlet.js: Medical Image Processing Plugins for the Web via JavaScript Injection},
  author={Gaibor, Edward and Varade, Shruti and Deshmukh, Rohini and Meyer, Tim and Geshvadi, Mahsa and Kim, SangHyuk and Narayanappa, Vidhya Sree and \myname{Haehn, Daniel}},
  abstract={Can web-based image processing and visualization tools easily integrate into existing websites without significant time and effort? Our Boostlet.js library addresses this challenge by providing an open-source, JavaScript-based web framework to enable additional image processing functionalities. Boostlet examples include kernel filtering, image captioning, data visualization, segmentation, and web-optimized machine-learning models. To achieve this, Boostlet.js uses a browser bookmark to inject a user-friendly plugin selection tool called PowerBoost into any host website. Boostlet provides on-site access to a standard API independent of any visualization framework for pixel data and scene manipulation. Boostlet is natively integrated into SliceDrop, a web-based, interactive viewer for medical imaging data. Boostlets provide a modular architecture and client-side processing capabilities to apply advanced image-processing techniques using consumer-level hardware. The code is open-source and available.},
  booktitle={IEEE International Symposium on Biomedical Imaging (ISBI)},
  pages={TBA},
  year={2025},
  organization={IEEE},
  code={https://github.com/mpsych/boostlet/},
  data={https://github.com/mpsych/boostlet/},
  website={https://boostlet.org},
  shortvenue={ISBI 2025}
}


@article{kim2025automatic,
title={Automatic Segmentation of Calcified Plaque in Carotid Arteries},
  author={Kim, Jiehyun and Wang, Kevin and Sakai, Yu and Zhu, Youxiang and Hu, Andrew C. and Phi, Huy Q. and Arnett, Nathan and Wang, Grace J. and Cucchiara, Brett L. and Song, Jae W. and \myname{Haehn, Daniel}},
  abstract={Stroke remains a leading cause of death globally, with calcified plaque in the carotid artery being a significant risk factor. Evaluating the impact of calcified carotid plaque, particularly in patients with embolic stroke of undetermined source (ESUS), is critical yet challenging. Manual segmentation, essential for assessing stroke risk, is time-consuming, and conventional methods like 2D and 3D UNet often struggle with the small size of calcified plaques. 
Therefore, this paper introduces a two-step segmentation process. First, segments the carotid artery to narrow the search space and focus on the region of interest around the artery. Then, it segments the calcified plaque within that targeted region. This approach achieves an intersection over union (IoU) of 0.9412 for the 2D model and 0.8095 for the 3D model, outperforming the baseline methods that directly segment plaques. All developments are open source and publicly accessible on our GitHub: https://github.com/mpsych/CACTAS-AI.},
  booktitle={IEEE International Symposium on Biomedical Imaging (ISBI)},
  pages={TBA},
  year={2025},
  organization={IEEE},
  code={https://github.com/jiehyunjkim/CACTAS-AI},
  data={https://github.com/jiehyunjkim/CACTAS-AI},
  shortvenue={ISBI 2025}
}


%
% 2024
%
@inproceedings{zhu2024adversarial,
    title = "Adversarial Text Generation using Large Language Models for Dementia Detection",
    author = "Zhu, Youxiang  and
      Lin, Nana  and
      Balivada, Kiran Sandilya  and
      Haehn, Daniel  and
      Liang, Xiaohui",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1222/",
    doi = "10.18653/v1/2024.emnlp-main.1222",
    pages = "21918--21933",
    abstract = "Although large language models (LLMs) excel in various text classification tasks, regular prompting strategies (e.g., few-shot prompting) do not work well with dementia detection via picture description. The challenge lies in the language marks for dementia are unclear, and LLM may struggle with relating its internal knowledge to dementia detection. In this paper, we present an accurate and interpretable classification approach by Adversarial Text Generation (ATG), a novel decoding strategy that could relate dementia detection with other tasks. We further develop a comprehensive set of instructions corresponding to various tasks and use them to guide ATG, achieving the best accuracy of 85{\%}, {\ensuremath{>}}10{\%} improvement compared to the regular prompting strategies. In addition, we introduce feature context, a human-understandable text that reveals the underlying features of LLM used for classifying dementia. From feature contexts, we found that dementia detection can be related to tasks such as assessing attention to detail, language, and clarity with specific features of the environment, character, and other picture content or language-related features. Future work includes incorporating multi-modal LLMs to interpret speech and picture information.",
  entrysubtype = {fromUMB}
}

@inproceedings{meyer2024webgl,
  title={WebGL-based Image Processing through JavaScript Injection},
  author={Meyer, Tim and Dreo Rodosek, Gabi and \myname{Haehn, Daniel}},
  abstract={Can we modify existing web-based computer graphics content through JavaScript injection? We study how to hijack the WebGL context of any external website to perform GPU-accelerated image processing and scene modification. This allows client-side modification of 2D and 3D content without access to the web server. We demonstrate how JavaScript can overload an existing WebGL context and present examples such as color replacement, edge detection, image filtering, and complete visual transformations of external websites, as well as vertex and geometry processing and manipulation. We discuss the potential of such an approach and present open-source software for real-time processing using a bookmarklet implementation.},
  booktitle={ACM Conference on 3D Web Technology},
  pages={1--5},
  year={2024},
  website={https://meyerstim.github.io/WebGL-Image-Processor/},
  code={https://github.com/meyerstim/WebGL-Image-Processor},
  shortvenue={ACM Web3D 2024},
  entrysubtype = {fromUMB}
}

@article{cui2024generalization,
  title={Generalization of CNNs on Relational Reasoning With Bar Charts},
  author={Cui, Zhenxing and Chen, Lu and Wang, Yunhai and \myname{Haehn, Daniel} and Wang, Yong and Pfister, Hanspeter},
  abstract={This paper presents a systematic study of the generalization of convolutional neural networks (CNNs) and humans on relational reasoning tasks with bar charts. We first revisit previous experiments on graphical perception and update the benchmark performance of CNNs. We then test the generalization performance of CNNs on a classic relational reasoning task: estimating bar length ratios in a bar chart, by progressively perturbing the standard visualizations. We further conduct a user study to compare the performance of CNNs and humans. Our results show that CNNs outperform humans only when the training and test data have the same visual encodings. Otherwise, they may perform worse. We also find that CNNs are sensitive to perturbations in various visual encodings, regardless of their relevance to the target bars. Yet, humans are mainly influenced by bar lengths. Our study suggests that robust relational reasoning with visualizations is challenging for CNNs. Improving CNNs' generalization performance may require training them to better recognize task-related visual properties.},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2024},
  publisher={IEEE},
  website={https://ieeexplore.ieee.org/document/10684131},
  entrysubtype = {fromUMB},
  shortvenue={IEEE TVCG Journal}
}

@inproceedings{potasznik2024just,
  title={'Just let me': Experiences of Historically Marginalized Students in the CS Department},
  author={Potasznik, Amanda R. and Durupinar, Funda and \myname{Haehn, Daniel}},
  abstract={The University of Massachusetts in Boston (UMB) is the most ethnically diverse public university campus in the New England region of the United States. Student enrollment in the Computer Science (CS) department closely mirrors the ethnicity-related demographic statistics of the university as a whole, with more than half of CS majors identifying as students of color, bucking national trends. Students who identify as male in our department outweigh those who identify as female five to one, however: a ratio that is in line with national and global trends. These two phenomena place our department in an interesting place for diversity, equity, and inclusion research: we boast extremely strong inclusion rates for students of color in CS, but still struggle to recruit and retain women in our programs. An exploratory phenomenological case study of participatory action research was used to investigate the lived experiences of students from various backgrounds and gender identities within our department. Salient thematic codes were analyzed with dual purposes: 1) informing suggested improvements for the department for recruiting and retaining students of color and women, and 2) ensuring student participation and inclusion in the creation of new course evaluation questions that aim to gather more data about students' experiences across the entire department. Salient themes from the focus group included gender-neutral and ethnicity-neutral based experiences, the importance of female advocates and mentors, and international status concerns.},
  booktitle={IEEE Black Issues in Comp. Education (BICE)},
  pages={33--39},
  year={2024},
  organization={Springer, Cham},
  website={https://ieeexplore.ieee.org/document/10605866},
  shortvenue={IEEE BICE 2024},
  entrysubtype = {fromUMB}
}

@inproceedings{franke2024slicertms,
  title={SlicerTMS: Real-Time Visualization of Transcranial Magnetic Stimulation for Mental Health Treatment},
  author={Franke, Loraine and Park, Tae Young and Luo, Jie and Rathi, Yogesh and Pieper, Steve and Ning, Lipeng and \myname{Haehn, Daniel}},
  abstract={We present a real-time visualization system for Transcranial Magnetic Stimulation (TMS), a non-invasive neuromodulation technique for treating various brain disorders and mental health diseases. Our solution targets the current challenges of slow and labor-intensive practices in treatment planning. Integrating Deep Learning (DL), our system rapidly predicts electric field (E-field) distributions in 0.2 seconds for precise and effective brain stimulation. The core advancement lies in our tool’s real-time neuronavigation visualization capabilities, which support clinicians in making more informed decisions quickly and effectively. We assess our system’s performance through three studies: First, a real-world use case scenario in a clinical setting, providing concrete feedback on applicability and usability in a practical environment. Second, a comparative analysis with another TMS tool focusing on computational efficiency across various hardware platforms. Lastly, we conducted an expert user study to measure usability and influence in optimizing TMS treatment planning. The system is openly available for community use and further development on GitHub: https://github.com/lorifranke/SlicerTMS.},
  booktitle={Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  pages={XXX--XXX},
  year={2024},
  organization={Springer, Cham},
  code={https://github.com/lorifranke/slicertms},
  data={https://github.com/lorifranke/slicertms},
  website={https://github.com/lorifranke/slicertms},
  spotlight={yes},
  shortvenue={MICCAI 2024}
}

@article{kyeremah2024single,
  title={Single-beam digital holographic reconstruction: a phase-support enhanced complex wavefront on phase-only function for twin-image elimination},
  author={Kyeremah, Charlotte and Weiss, Matthew and Kandel, Dila and \myname{Haehn, Daniel} and Yelleswarapu, Chandra},
  abstract={Significance: In in-line digital holographic microscopy (DHM), twin-image artifacts pose a significant challenge, and reduction or complete elimination is essential for object reconstruction.

Aim: To facilitate object reconstruction using a single hologram, significantly reduce inaccuracies, and avoid iterative processing, a digital holographic reconstruction algorithm called phase-support constraint on phase-only function (PCOF) is presented.

Approach: In-line DHM simulations and tabletop experiments employing the sliding-window approach are used to compute the arithmetic mean and variance of the phase values in the reconstructed image. A support constraint mask, through variance thresholding, effectively enabled twin-image artifacts.

Results: Quantitative evaluations using metrics such as mean squared error, peak signal-to-noise ratio, and mean structural similarity index show PCOF's superior capability in eliminating twin-image artifacts and achieving high-fidelity reconstructions compared with conventional methods such as angular spectrum and iterative phase retrieval methods.

Conclusions: PCOF stands as a promising approach to in-line digital holographic reconstruction, offering a robust solution to mitigate twin-image artifacts and enhance the fidelity of reconstructed objects.},
  journal={Journal of Biomedical Optics},
  volume={29},
  number={7},
  pages={076502--076502},
  year={2024},
  publisher={Society of Photo-Optical Instrumentation Engineers},
  website={https://doi.org/10.1117/1.JBO.29.7.076502},
  code={https://github.com/Charly-KD/Ph.D-Work/tree/main},
  entrysubtype = {fromUMB}
}

@article{franke2024autorlx,
author = {Franke, Loraine and Weidele, Daniel Karl I. and Dehmamy, Nima and Ning, Lipeng and \myname{Haehn, Daniel}},
title = {AutoRL X: Automated Reinforcement Learning on the Web},
year = {2024},
issue_date = {TBA},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {TBA},
number = {TBA},
issn = {2160-6455},
url = {TBA},
doi = {TBA},
abstract = {Reinforcement Learning (RL) is crucial in decision optimization, but its inherent complexity often presents challenges in interpretation and communication. Building upon AutoDOViz — an interface that pushed the boundaries of Automated RL for Decision Optimization — this paper unveils an open-source expansion with a web-based platform for RL. Our work introduces a taxonomy of RL visualizations and launches a dynamic web platform, leveraging backend flexibility for AutoRL frameworks like ARLO and Svelte.js for a smooth interactive user experience in the front end. Since AutoDOViz is not open-source, we present AutoRL X, a new interface designed to visualize RL processes. AutoRL X is shaped by the extensive user feedback and expert interviews from AutoDOViz studies, and it brings forth an intelligent interface with real-time, intuitive visualization capabilities that enhance understanding, collaborative efforts, and personalization of RL agents. Addressing the gap in accurately representing complex real-world challenges within standard RL environments, we demonstrate our tool's application in healthcare, explicitly optimizing brain stimulation trajectories. A user study contrasts the performance of human users optimizing electric fields via a 2D interface with RL agents' behavior that we visually analyze in AutoRL X, assessing the practicality of automated RL. All our data and code is available online at: https://github.com/lorifranke/autorlx},
journal = {ACM Transactions Interactive Intelligent Systems},
month = {TBA},
articleno = {TBA},
numpages = {TBA},
code={https://github.com/lorifranke/autorlx},
data={https://github.com/lorifranke/autorlx},
shortvenue={ACM TIIS Journal}
}

@article{tiwari2024noninvasive,
title={Non-invasive Stress Monitoring from Video},
  author={Tiwari, Akshata and Matejek, Brian and \myname{Haehn, Daniel}},
  abstract={Identifying stress is crucial for maintaining a healthy lifestyle.
Current stress-detection methods are relatively slow and subjective and often take place through invasive measurements via medical devices. We instead propose end-to-end, non-invasive detection of stress through video. We incorporate several modalities to perform holistic detection of a user’s stress level. Our framework employs an emotion recognition model to detect expressions through facial recordings. Then, we evaluate a user's heart rate by amplifying changes in skin coloration through Eulerian Video Magnification. Finally, we analyze differentials in eyebrow and lip movements. We combine these three measurements to output a final stress score per unit of time. Our chosen emotion recognition model achieves an accuracy of 96.46%, and our remote heart rate detection module has a mean absolute error of 5.79 BPM. We provide a web-based application that allows for rapid, contactless stress detection through a webcam. We achieve over 84% accuracy on a dataset of video clips with individuals labeled as undergoing low, moderate, or high-stress levels. All code and data are openly available.},
  booktitle={IEEE International Symposium on Biomedical Imaging (ISBI)},
  pages={TBA},
  year={2024},
  organization={IEEE},
  code={https://github.com/mpsych/stress},
  data={https://github.com/mpsych/stress},
  shortvenue={ISBI 2024}
}


@article{park2024review,
  title={A review of algorithms and software for real-time electric field modeling techniques for transcranial magnetic stimulation},
  author={Park, Tae Young and Franke, Loraine and Pieper, Steve and \myname{Haehn, Daniel} and Ning, Lipeng},
  journal={Biomedical Engineering Letters},
  pages={1--13},
  year={2024},
  publisher={Springer},
  abstract={Transcranial magnetic stimulation (TMS) is a device-based neuromodulation technique increasingly used to treat brain diseases. Electric field (E-field) modeling is an important technique in several TMS clinical applications, including the precision stimulation of brain targets with accurate stimulation density for the treatment of mental disorders and the localization of brain function areas for neurosurgical planning. Classical methods for E-field modeling usually take a long computation time. Fast algorithms are usually developed with significantly lower spatial resolutions that reduce the prediction accuracy and limit their usage in real-time or near real-time TMS applications. This review paper discusses several modern algorithms for real-time or near real-time TMS E-field modeling and their advantages and limitations. The reviewed methods include techniques such as basis representation techniques and deep neural-network-based methods. This paper also provides a review of software tools that can integrate E-field modeling with navigated TMS, including a recent software for real-time navigated E-field mapping based on deep neural-network models.},
  DOI={10.1007/s13534-024-00373-4},
  url={https://link.springer.com/article/10.1007/s13534-024-00373-4},
  shortvenue={Biomed. Engin. Letters}
}

%
% 2023
%

@Article{makaram2023deeplearning,
AUTHOR = {Makaram, Navaneethakrishna and Gupta, Sarvagya and Pesce, Matthew and Bolton, Jeffrey and Stone, Scellig and Haehn, Daniel and Pomplun, Marc and Papadelis, Christos and Pearl, Phillip and Rotenberg, Alexander and Grant, Patricia Ellen and Tamilia, Eleonora},
TITLE = {Deep Learning-Based Visual Complexity Analysis of Electroencephalography Time-Frequency Images: Can It Localize the Epileptogenic Zone in the Brain?},
JOURNAL = {MDPI Algorithms},
VOLUME = {16},
YEAR = {2023},
NUMBER = {12},
ARTICLE-NUMBER = {567},
URL = {https://www.mdpi.com/1999-4893/16/12/567},
ISSN = {1999-4893},
ABSTRACT = {In drug-resistant epilepsy, a visual inspection of intracranial electroencephalography (iEEG) signals is often needed to localize the epileptogenic zone (EZ) and guide neurosurgery. The visual assessment of iEEG time-frequency (TF) images is an alternative to signal inspection, but subtle variations may escape the human eye. Here, we propose a deep learning-based metric of visual complexity to interpret TF images extracted from iEEG data and aim to assess its ability to identify the EZ in the brain. We analyzed interictal iEEG data from 1928 contacts recorded from 20 children with drug-resistant epilepsy who became seizure-free after neurosurgery. We localized each iEEG contact in the MRI, created TF images (1–70 Hz) for each contact, and used a pre-trained VGG16 network to measure their visual complexity by extracting unsupervised activation energy (UAE) from 13 convolutional layers. We identified points of interest in the brain using the UAE values via patient- and layer-specific thresholds (based on extreme value distribution) and using a support vector machine classifier. Results show that contacts inside the seizure onset zone exhibit lower UAE than outside, with larger differences in deep layers (L10, L12, and L13: p < 0.001). Furthermore, the points of interest identified using the support vector machine, localized the EZ with 7 mm accuracy. In conclusion, we presented a pre-surgical computerized tool that facilitates the EZ localization in the patient’s MRI without requiring long-term iEEG inspection.},
DOI = {10.3390/a16120567},
shortvenue={MDPI Algorithms}
}

@article{qi2023lesion,
  title={Lesion Search with Self-supervised Learning},
  author={Qi, Kristin and Cheng, Jiali and \myname{Haehn, Daniel}},
  journal={International Conference on Learning Representations (ICLR)},
  abstract={Content-based image retrieval (CBIR) with self-supervised learning (SSL) accelerates clinicians’ interpretation of similar images without manual annotations. We develop a CBIR from the contrastive learning SimCLR and incorporate a generalized-mean (GeM) pooling followed by L2 normalization to classify lesion types and retrieve similar images before clinicians' analysis. Results have shown improved performance. We additionally build an open-source application for image analysis and retrieval. The application is easy to integrate, relieving manual efforts and suggesting the potential to support clinicians’ everyday activities.},
  code={https://github.com/openhcimed/flask_search},
  year={2023},
  shortvenue={ICLR 2023}
}


@inproceedings{zurrin2023outlier,
title={Outlier Detection for Mammograms},
author={Ryan Zurrin and Neha Goyal and Pablo Bendiksen and Muskaan Manocha and Dan Simovici and Nurit Haspel and Marc Pomplun and \myname{Daniel Haehn}},
abstract={Mammograms are vital for detecting breast cancer, the most common cancer among women in the US. However, low-quality scans and imaging artifacts can compromise their efficacy. We introduce an automated pipeline to filter low-quality mammograms from large datasets. Our initial dataset of 176,492 mammograms contained an estimated 5.5% lower quality scans with issues like metal coil frames, wire clamps, and breast implants. Manually removing these images is tedious and error-prone. Our two-stage process first uses threshold-based 5-bin histogram filtering to eliminate undesirable images, followed by a variational autoencoder to remove remaining low-quality scans. Our method detects such scans with an F1 Score of 0.8862 and preserves 163,568 high-quality mammograms. We provide results and tools publicly available as open-source software.},
booktitle={International Conference on Medical Imaging with Deep Learning (MIDL)},
year={2023},
url={https://openreview.net/forum?id=4E93Xdg98u},
code={https://github.com/mpsych/ODM},
shortvenue={MIDL 2023}
}

@inproceedings{dehghanpoor2023classifying,
  title={Classifying Protein Families with Learned Compressed Representations},
  author={Dehghanpoor, Ramin and Afrasiabi, Fatemeh and Fogel, Charles and Dao, Tung and Gautam, Suman and Nehela, Aanab and Nehela, Ahmad and Haehn, Daniel and Haspel, Nurit},
  booktitle={International Conference on Bioinformatics},
  abstract={Classifying proteins into families is an important task when studying newly discovered proteins. If we can identify the family a protein belongs to, we can predict features without knowing the exact structure of such a protein. However, this grouping process is challenging. We propose a two-stage algorithm that classifies proteins into families by combining a dimensionality reduction technique using a variational autoencoder with learned fingerprint representations using a Convolutional Neural Network (CNN). Our models use fewer parameters than existing methods but perform better, with our variational autoencoder achieving 94% accuracy in reconstructing the most common amino acid in a sequence alignment, and the neural network provides 98-100% accuracy in classifying protein families. We developed a software framework to access our algorithms. All code and data are publicly available at https://github.com/ramindehghanpoor/CLI.},
  volume={92},
  pages={47--57},
  year={2023},
  code={https://github.com/ramindehghanpoor/CLI},
  data={https://github.com/ramindehghanpoor/CLI},
  bestpaper={yes},
  shortvenue={BICOB 2023 (Best Paper Award)}
}

@inproceedings{weidele2023autodoviz,
author = {Weidele, Daniel Karl I. and Afzal, Shazia and Valente, Abel N. and Makuch, Cole and Cornec, Owen and Vu, Long and Subramanian, Dharmashankar and Geyer, Werner and Nair, Rahul and Vejsbjerg, Inge and Marinescu, Radu and Palmes, Paulito and Daly, Elizabeth M. and Franke, Loraine and Haehn, Daniel},
title = {AutoDOViz: Human-Centered Automation for Decision Optimization},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584094},
doi = {10.1145/3581641.3584094},
abstract = {We present AutoDOViz, an interactive user interface for automated decision optimization (AutoDO) using reinforcement learning (RL). Decision optimization (DO) has classically being practiced by dedicated DO researchers [43] where experts need to spend long periods of time fine tuning a solution through trial-and-error. AutoML pipeline search has sought to make it easier for a data scientist to find the best machine learning pipeline by leveraging automation to search and tune the solution. More recently, these advances have been applied to the domain of AutoDO [36], with a similar goal to find the best reinforcement learning pipeline through algorithm selection and parameter tuning. However, Decision Optimization requires significantly more complex problem specification when compared to an ML problem. AutoDOViz seeks to lower the barrier of entry for data scientists in problem specification for reinforcement learning problems, leverage the benefits of AutoDO algorithms for RL pipeline search and finally, create visualizations and policy insights in order to facilitate the typical interactive nature when communicating problem formulation and solution proposals between DO experts and domain experts. In this paper, we report our findings from semi-structured expert interviews with DO practitioners as well as business consultants, leading to design requirements for human-centered automation for DO with RL. We evaluate a system implementation with data scientists and find that they are significantly more open to engage in DO after using our proposed solution. AutoDOViz further increases trust in RL agent models and makes the automated training and evaluation process more comprehensible. As shown for other automation in ML tasks [33, 59], we also conclude automation of RL for DO can benefit from user and vice-versa when the interface promotes human-in-the-loop.},
booktitle = {International Conference on Intelligent User Interfaces},
pages = {664–680},
numpages = {17},
keywords = {automation, reinforcement learning, decision optimization},
location = {Sydney, NSW, Australia},
series = {IUI '23},
website={https://dl.acm.org/doi/abs/10.1145/3581641.3584094},
shortvenue={IUI 2023}
}


%
% 2022
%
@InProceedings{goyal2022realtime,
author={Goyal, Neha and Hussain, Yahiya and Yang, Gianna G. and Haehn, Daniel},
title={Real-Time Alignment for Connectomics},
booktitle={Biomedical Image Registration (WBIR)},
year={2022},
publisher={Springer International Publishing},
address={Cham},
pages={211--214},
abstract={In Connectomics, researchers are creating the brain's wiring diagram at nanometer resolution. As part of this processing workflow, 2D electron microscopy (EM) images must be aligned to 3D volumes. However, existing alignment methods are computationally expensive and can take a long time. We hypothesize that adding biological features improve and accelerate the alignment procedure. Since especially mitochondria can be detected accurately and fast, we propose a new alignment method, MITO, that uses these structures as landmark points. With MITO, we can decrease the alignment time by 27%, and our experiments indicate a throughput of 33 Megapixels/s, which is faster than the acquisition speed of current microscopes. We can align an image volume of 1268x1524x160 voxels in less than 12s. We compare our method to the following feature generators: ORB, BRISK, FAST, and FREAK.},
isbn={978-3-031-11203-4},
code={https://github.com/nehagoyal1994/mito},
website={https://link.springer.com/chapter/10.1007/978-3-031-11203-4_25},
data={https://github.com/nehagoyal1994/mito},
shortvenue={WBIR 2022},
poster={https://mpsych.org/papers/goyal2022realtime_poster.pdf}
}

@ARTICLE{burkhardt2022ntools,
AUTHOR={Burkhardt, Jay and Sharma, Aaryaman and Tan, Jack and Franke, Loraine and Leburu, Jahnavi and Jeschke, Jay and Devore, Sasha and Friedman, Daniel and Chen, Jingyun and Haehn, Daniel},   
TITLE={N-Tools-Browser: Web-Based Visualization of Electrocorticography Data for Epilepsy Surgery},
JOURNAL={Frontiers in Bioinformatics},      
VOLUME={2},      
YEAR={2022},      
URL={https://www.frontiersin.org/article/10.3389/fbinf.2022.857577},       
DOI={10.3389/fbinf.2022.857577},      
ISSN={2673-7647},   
ABSTRACT={Epilepsy affects more than three million people in the United States. In approximately one-third of this population, anti-seizure medications do not control seizures. Many patients pursue surgical treatment that can include a procedure involving the implantation of electrodes for intracranial monitoring of seizure activity. For these cases, accurate mapping of the implanted electrodes on a patient’s brain is crucial in planning the ultimate surgical treatment. Traditionally, electrode mapping results are presented in static figures that do not allow for dynamic interactions and visualizations. In collaboration with a clinical research team at a Level 4 Epilepsy Center, we developed N-Tools-Browser, a web-based software using WebGL and the X-Toolkit (XTK), to help clinicians interactively visualize the location and functional properties of implanted intracranial electrodes in 3D. Our software allows the user to visualize the seizure focus location accurately and simultaneously display functional characteristics (e.g., results from electrical stimulation mapping). Different visualization modes enable the analysis of multiple electrode groups or individual anatomical locations. We deployed a prototype of N-Tools-Browser for our collaborators at the New York University Grossman School of Medicine Comprehensive Epilepsy Center. Then, we evaluated its usefulness with domain experts on clinical cases.},
code={https://github.com/ntoolsbrowser/ntoolsbrowser.github.io/},
website={https://ntoolsbrowser.github.io/},
data={https://github.com/ntoolsbrowser/ntoolsbrowser.github.io/tree/main/data},
shortvenue={Frontiers in Bioinformatics}
}

@article{paulick2022promoting,
AUTHOR = {Paulick, Katharina and Seidel, Simon and Lange, Christoph and Kemmer, Annina and Cruz-Bournazou, Mariano Nicolas and Baier, André and Haehn, Daniel},
TITLE = {Promoting Sustainability through Next-Generation Biologics Drug Development},
JOURNAL = {MDPI Sustainability},
VOLUME = {14},
YEAR = {2022},
NUMBER = {8},
ARTICLE-NUMBER = {4401},
URL = {https://www.mdpi.com/2071-1050/14/8/4401},
ISSN = {2071-1050},
ABSTRACT = {The fourth industrial revolution in 2011 aimed to transform the traditional manufacturing processes. As part of this revolution, disruptive innovations in drug development and data science approaches have the potential to optimize CMC (chemistry, manufacture, and control). The real-time simulation of processes using &ldquo;digital twins&rdquo; can maximize efficiency while improving sustainability. As part of this review, we investigate how the World Health Organization&rsquo;s 17 sustainability goals can apply toward next-generation drug development. We analyze the state-of-the-art laboratory leadership, inclusive personnel recruiting, the latest therapy approaches, and intelligent process automation. We also outline how modern data science techniques and machine tools for CMC help to shorten drug development time, reduce failure rates, and minimize resource usage. Finally, we systematically analyze and compare existing approaches to our experiences with the high-throughput laboratory KIWI-biolab at the TU Berlin. We describe a sustainable business model that accelerates scientific innovations and supports global action toward a sustainable future.},
DOI = {10.3390/su14084401},
shortvenue={MDPI Sustainability}
}


@article{singh2022how,
  doi = {10.1007/s12021-022-09572-9},
  url = {https://doi.org/10.1007/s12021-022-09572-9},
  year = {2022},
  month = {mar},
  publisher = {Springer Science and Business Media {LLC}},
  author = {Nalini M. Singh and Jordan B. Harrod and Sandya Subramanian and Mitchell Robinson and Ken Chang and Suheyla Cetin-Karayumak and Adrian Vasile Dalca and Simon Eickhoff and Michael Fox and Loraine Franke and Polina Golland and Daniel Haehn and Juan Eugenio Iglesias and Lauren J. O'Donnell and Yangming Ou and Yogesh Rathi and Shan H. Siddiqi and Haoqi Sun and M. Brandon Westover and Susan Whitfield-Gabrieli and Randy L. Gollub},
  abstract = {This report presents an overview of how machine learning is rapidly advancing clinical translational imaging in ways that will aid in the early detection, prediction, and treatment of diseases that threaten brain health. Towards this goal, we aresharing the information presented at a symposium, “Neuroimaging Indicators of Brain Structure and Function - Closing the Gap Between Research and Clinical Application”, co-hosted by the McCance Center for Brain Health at Mass General Hospital and the MIT HST Neuroimaging Training Program on February 12, 2021. The symposium focused on the potential for machine learning approaches, applied to increasingly large-scale neuroimaging datasets, to transform healthcare delivery and change the trajectory of brain health by addressing brain care earlier in the lifespan. While not exhaustive, this overview uniquely addresses many of the technical challenges from image formation, to analysis and visualization, to synthesis and incorporation into the clinical workflow. Some of the ethical challenges inherent to this work are also explored, as are some of the regulatory requirements for implementation. We seek to educate, motivate, and inspire graduate students, postdoctoral fellows, and early career investigators to contribute to a future where neuroimaging meaningfully contributes to the maintenance of brain health.},
  title = {How Machine Learning is Powering Neuroimaging to Improve Brain Health},
  journal = {Neuroinformatics},
  website={https://link.springer.com/article/10.1007/s12021-022-09572-9},
  shortvenue={Neuroinformatics}
}


%
% 2021
%
@article {baidak2021cellprofiler,
    title={CellProfiler Analyst Web (CPAW) - Exploration, analysis, and classification of biological images on the web},
  author={Baidak, Bella and Hussain, Yahiya and Kelminson, Emma and Jones, Thouis R. and Franke, Loraine and \myname{Haehn, Daniel}},
    abstract={CellProfiler Analyst (CPA) has enabled the scientific research community to explore image-based data and classify complex biological phenotypes through an interactive user interface since its release in 2008. This paper describes CellProfiler Analyst Web (CPAW), a newly redesigned and web-based version of the software, allowing for greater accessibility, quicker setup, and facilitating a simple workflow for users. Installation and managing new versions has been challenging and time-consuming, historically. CPAW is an alternative that ensures installation and future updates are not a hassle to the user. CPAW ports the core iteration loop of CPA to a pure server-less browser environment using modern web-development technologies, allowing computationally heavy activities, like machine learning, to occur without freezing the user interface (UI). With a setup as simple as navigating to a website, CPAW presents a clean UI to the user to refine their classifier and explore phenotypic data easily. We evaluated both the old and the new version of the software in an extensive domain expert study. We found that users could complete the essential classification tasks in CPAW and CPA 3.0 with the same efficiency. Additionally, users completed the tasks 20 percent faster using CPAW compared to CPA 3.0. The code of CellProfiler Analyst Web is open-source and available at https://mpsych.github.io/CellProfilerAnalystWeb/.},
    journal={IEEE Visualization Short Paper (IEEE VIS)},
    year={2021},
    code={https://github.com/mpsych/CellProfilerAnalystWeb},
    website={https://mpsych.github.io/CellProfilerAnalystWeb/},
    data={http://d1zymp9ayga15t.cloudfront.net/content/Examplezips/cpa_2.0_example.zip},
    shortvenue={IEEE VIS 2021 Short Paper}
}

@article {franke2021fiberstars,
    title={FiberStars: Visual Comparison of Diffusion Tractography Data between Multiple Subjects},
  author={Franke, Loraine and Weidele, Daniel Karl I and Zhang, Fan and Cetin-Karayumak, Suheyla and Pieper, Steve and O'Donnell, Lauren J and Rathi, Yogesh and \myname{Haehn, Daniel}},
    abstract={Tractography from high-dimensional diffusion magnetic resonance imaging (dMRI) data allows brain’s structural connectivity analysis. Recent dMRI studies aim to compare connectivity patterns across subject groups and disease populations to understand subtle abnormalities in the brain’s white matter connectivity and distributions of biologically sensitive dMRI derived metrics. Existing software products focus solely on the anatomy, are not intuitive or restrict the comparison of multiple subjects. In this paper, we present the design and implementation of FiberStars, a visual analysis tool for tractography data that allows the interactive visualization of brainfiber clusters combining existing 3D anatomy with compact 2D visualizations. With FiberStars, researchers can analyze and compare multiple subjects in large collections of brain fibers using different views.  To evaluate the usability of our software, we performed a quantitative user study. We asked domain experts and non-experts to find patterns in a tractography dataset with either FiberStars or an existing dMRI exploration tool. Our results show that participants using FiberStars can navigate extensive collections of tractography faster and more accurately. All our research, software, and results are available openly.},
    journal={IEEE Pacific Visualization (PacificVis)},
    year={2021},
    code={https://github.com/lorifranke/FiberStars},
    website={https://github.com/lorifranke/FiberStars},
    data={https://github.com/lorifranke/FiberStars/tree/master/public/data},
    shortvenue={PacificVis 2021}
}

%
% 2020
%
@inproceedings{franke2020modern,
  title={Modern Scientific Visualizations on the Web},
  author={Franke, Loraine and \myname{Haehn, Daniel}},
  booktitle={MDPI Informatics},
  volume={7},
  number={4},
  pages={37},
  year={2020},
  abstract={Modern scientific visualization is web-based and uses emerging technology such as WebGL (Web Graphics Library) and WebGPU for three-dimensional computer graphics and WebXR for augmented and virtual reality devices. These technologies, paired with the accessibility of websites, potentially offer a user experience beyond traditional standalone visualization systems. We review the state-of-the-art of web-based scientific visualization and present an overview of existing methods categorized by application domain. As part of this analysis, we introduce the Scientific Visualization Future Readiness Score (SciVis FRS) to rank visualizations for a technology-driven disruptive tomorrow. We then summarize challenges, current state of the publication trend, future directions, and opportunities for this exciting research field.},
  organization={Multidisciplinary Digital Publishing Institute},
  code={https://github.com/mpsych/SciVis-Web},
  data={https://github.com/mpsych/SciVis-Web},
  website={https://github.com/mpsych/SciVis-Web},
  shortvenue={MDPI Informatics 2020}
}

@inproceedings{haehn2020trako,
  title={TRAKO: Efficient Transmission of Tractography Data for Visualization},
  author={\myname{Haehn, Daniel} and Franke, Loraine and Zhang, Fan and Karayumak, Suheyla Cetin and Pieper, Steve and O'Donnell, Lauren and Rathi, Yogesh},
  abstract={Fiber tracking produces large tractography datasets that are tens of gigabytes in size consisting of millions of streamlines. Such vast amounts of data require formats that allow for efficient storage, transfer, and visualization. We present TRAKO, a new data format based on the Graphics Layer Transmission Format (glTF) that enables immediate graphical and hardware-accelerated processing. We integrate a state-of-the-art compression technique for vertices, streamlines, and attached scalar and property data. We then compare TRAKO to existing tractography storage methods and provide a detailed evaluation on eight datasets. TRAKO can achieve data reductions of over 28x without loss of statistical significance when used to replicate analysis from previously published studies. },
  booktitle={Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  pages={XXX--XXX},
  year={2020},
  supplemental={https://mpsych.org/papers/haehn2020trako_supplemental.pdf},
  organization={Springer, Cham},
  code={https://github.com/bostongfx/TRAKO/},
  data={https://github.com/bostongfx/TRAKO/},
  website={https://pypi.org/project/trako/},
  shortvenue={MICCAI 2020}
}

@article{lin2020twostream,
  title={Two Stream Active Query Suggestion for Active Learning in Connectomics},
  author={Lin, Zudi and Wei, Donglai and Jang, Won-Dong and Zhou, Siyan and Chen, Xupeng and Wang, Xueying and Schalek, Richard and Berger, Daniel and Matejek, Brian and Kamentsky, Lee and Peleg, Adi and \myname{Haehn, Daniel} and Jones, Thouis R. and Parag, Toufiq and Lichtman, Jeff and Pfister, Hanspeter},
  abstract={For large-scale vision tasks in biomedical images, the labeled data is often limited to train effective deep models. Active learning is a common solution, where a query suggestion method selects representative unlabeled samples for annotation, and the new labels are used to improve the base model. However, most query suggestion models optimize their learnable parameters only on the limited labeled data and consequently become less effective for the more challenging unlabeled data. To tackle this, we propose a two-stream active query suggestion approach. In addition to the supervised feature extractor, we introduce an unsupervised one optimized on all raw images to capture diverse image features, which can later be improved by fine-tuning on new labels. As a use case, we build an end-to-end active learning framework with our query suggestion method for 3D synapse detection and mitochondria segmentation in connectomics. With the framework, we curate, to our best knowledge, the largest connectomics dataset with dense synapses and mitochondria annotation.},
  booktitle = {European Conference on Computer Vision (ECCV)},
  month = {August},
  year = {2020},
  supplemental={https://mpsych.org/papers/lin2020twostream_supplemental.pdf},
  code={https://github.com/zudi-lin/pytorch_connectomics/},
  data={https://zudi-lin.github.io/projects/},
  website={https://zudi-lin.github.io/projects/#two_stream_active},
  shortvenue={ECCV 2020}
}

@article {lekschas2020peax,
    title={Peax: Interactive Visual Pattern Search in Sequential Data Using Unsupervised Deep Representation Learning},
    author={Lekschas, Fritz and Peterson, Brant and Haehn, Daniel and Ma, Eric and Gehlenborg, Nils and Pfister, Hanspeter},
    abstract={We present Peax, a novel feature-based technique for interactive visual pattern search in sequential data, like time series or data mapped to a genome sequence. Visually searching for patterns by similarity is often challenging because of the large search space, the visual complexity of patterns, and the user’s perception of similarity. For example, in genomics, researchers try to link patterns in multivariate sequential data to cellular or pathogenic processes, but a lack of ground truth and high variance makes automatic pattern detection unreliable. We have developed a convolutional autoencoder for unsupervised representation learning of regions in sequential data that can capture more visual details of complex patterns compared to existing similarity measures. Using this learned representation as features of the sequential data, our accompanying visual query system enables interactive feedback-driven adjustments of the pattern search to adapt to the users’ perceived similarity. Using an active learning sampling strategy, Peax collects user-generated binary relevance feedback. This feedback is used to train a model for binary classification, to ultimately find other regions that exhibit patterns similar to the search target. We demonstrate Peax’s features through a case study in genomics and report on a user study with eight domain experts to assess the usability and usefulness of Peax. Moreover, we evaluate the effectiveness of the learned feature representation for visual similarity search in two additional user studies. We find that our models retrieve significantly more similar patterns than other commonly used techniques.},
    journal={Computer Graphics Forum},
    publisher={The Eurographics Association and John Wiley & Sons Ltd.},
    year={2020},
    bestpaper={yes},
    issn={1467-8659},
    doi={10.1111/cgf.13971},
    supplemental={https://mpsych.org/papers/lekschas2020peax_supplemental.pdf},
    code={https://github.com/flekschas/peax-experiment},
    data={https://github.com/flekschas/peax-experiment},
    website={http://peax.lekschas.de/},
    shortvenue={EuroVis 2020 (Best Paper Award)}
}

@inproceedings{casser2020fast,
  title={Fast Mitochondria Detection for Connectomics},
  author={Casser, Vincent and Kang, Kai and Pfister, Hanspeter and \myname{Daniel Haehn}},
  abstract={High-resolution connectomics data allows for the identification of dysfunctional mitochondria which are linked to a variety of diseases such as autism or bipolar. However, manual analysis is not feasible since datasets can be petabytes in size. We present a fully automatic mitochondria detector based on a modified U-Net architecture that yields high accuracy and fast processing times. We evaluate our method on multiple real-world connectomics datasets, including an improved version of the EPFL mitochondria benchmark. Our results show an Jaccard index of up to 0.90 with inference times lower than 16ms for a 512x512px image tile. This speed is faster than the acquisition speed of modern electron microscopes, enabling mitochondria detection in real-time. Our detector ranks first for real-time detection when compared to previous works and data, results, and code are openly available.},
  booktitle={International Conference on Medical Imaging with Deep Learning},
  pages={XXX--XXX},
  year={2020},
  spotlight={yes},
  website={https://sites.google.com/view/connectomics/},
  code={https://github.com/mpsych/mitochondria/},
  data={https://sites.google.com/view/connectomics/},
  shortvenue={MIDL 2020 (Spotlight)}
}


%
% 2019
%
@InProceedings{matejek2019biologically,
    title={Biologically-Constrained Graphs for Global Connectomics Reconstruction},
    author={Matejek, Brian and \myname{Haehn, Daniel} and Zhu, Haidong and Wei, Donglai and Parag, Toufiq and Pfister, Hanspeter},
    abstract={Most current state-of-the-art connectome reconstruction pipelines have two major steps: initial pixel-based segmentation with affinity prediction and watershed transform, and refined segmentation by merging over-segmented regions. These methods rely only on local context and are typically agnostic to the underlying biology. Since a few merge errors can lead to several incorrectly merged neuronal processes, these algorithms are currently tuned towards over-segmentation producing an overburden of costly proofreading. We propose a third step for connectomics reconstruction pipelines to refine an over-segmentation using both local and global context with an emphasis on adhering to the underlying biology. We first extract a graph from an input segmentation where nodes correspond to segment labels and edges indicate potential split errors in the over-segmentation. In order to increase throughput and allow for large-scale reconstruction, we employ biologically inspired geometric constraints based on neuron morphology to reduce the number of nodes and edges. Next, two neural networks learn these neuronal shapes to further aid the graph construction process. Lastly, we reformulate the region merging problem as a graph partitioning one to leverage global context. We demonstrate the performance of our approach on four real-world connectomics datasets with an average variation of information improvement of 21.3%.},
    booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019},
    code={https://github.com/bmatejek/ibex/},
    shortvenue={CVPR 2019}
}

%
% 2018
%
@article{haehn2018evaluating,
  title={Evaluating 'Graphical Perception' with CNNs},
  author={\myname{Haehn, Daniel} and Tompkin, James and Pfister, Hanspeter},
  journal={IEEE Transactions on Visualization and Computer Graphics (IEEE VIS)},
  abstract={Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill’s seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.},
  volume={25},
  pages={641--650},
  number={1},
  year={2018},
  month={October},
  publisher={IEEE},
  supplemental={http://mpsych.org/papers/haehn2018evaluating_supplemental.pdf},
  code={http://rhoana.org/perception/},
  data={http://rhoana.org/perception/},
  video={https://vimeo.com/280506639},
  fastforward={https://vimeo.com/285106317},
  poster={http://mpsych.org/papers/haehn2018evaluating_poster.pdf},
  shortvenue={IEEE VIS 2018}
}

@InProceedings{haehn2018guided,
    title={Guided Proofreading of Automatic Segmentations for Connectomics},
    author={\myname{Haehn, Daniel} and Kaynig, Verena and Tompkin, James and Lichtman, Jeff W. and Pfister, Hanspeter},
    abstract={Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.},
    booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2018},
    supplemental={http://mpsych.org/papers/haehn2018guided_supplemental.pdf},
    code={http://rhoana.org/guidedproofreading/},
    data={https://github.com/haehn/proofreading},
    video={https://vimeo.com/280507933},
    poster={http://mpsych.org/papers/haehn2018guided_poster.pdf},
    shortvenue={CVPR 2018}
}

%
% 2017
%
@article{haehn2017scalable,
  title={Scalable Interactive Visualization for Connectomics},
  author={\myname{Haehn, Daniel} and Hoffer, John and Matejek, Brian and Suissa-Peleg, Adi and Al-Awami, Ali K. and Kamentsky, Lee and Gonda, Felix and Meng, Eagon and Zhang, William and Schalek, Richard and Wilson, Alyssa and Parag, Toufiq and Beyer, Johanna and Kaynig, Verena and Jones, Thouis R. and Tompkin, James and Hadwiger, Markus and Lichtman, Jeff W. and Pfister, Hanspeter},
  journal={MDPI Informatics},
  abstract={Connectomics has recently begun to image brain tissue at nanometer resolution, which produces petabytes of data. This data must be aligned, labeled, proofread, and formed into graphs, and each step of this process requires visualization for human verification. As such, we present the BUTTERFLY middleware, a scalable platform that can handle massive data for interactive visualization in connectomics. Our platform outputs image and geometry data suitable for hardware-accelerated rendering, and abstracts low-level data wrangling to enable faster development of new visualizations. We demonstrate scalability and extendability with a series of open source Web-based applications for every step of the typical connectomics workflow: data management and storage, informative queries, 2D and 3D visualizations, interactive editing, and graph-based analysis. We report design choices for all developed applications and describe typical scenarios of isolated and combined use in everyday connectomics research. In addition, we measure and optimize rendering throughput—from storage to display—in quantitative experiments. Finally, we share insights, experiences, and recommendations for creating an open source data management and interactive visualization platform for connectomics.},
  volume={4},
  number={3},
  pages={29},
  year={2017},
  organization={Multidisciplinary Digital Publishing Institute},
  code={https://github.com/Rhoana/butterfly},
  video={https://vimeo.com/280509756},
  shortvenue={MDPI Informatics 2017}
}

@inproceedings{matejek2017compresso,
  title={Compresso: Efficient Compression of Segmentation Data For Connectomics},
  author={Matejek, Brian and \myname{Haehn, Daniel} and Lekschas, Fritz and Mitzenmacher, Michael and Pfister, Hanspeter},
  abstract={Recent advances in segmentation methods for connectomics and biomedical imaging produce very large datasets with labels that assign object classes to image pixels. The resulting label volumes are bigger than the raw image data and need compression for efficient storage and transfer. General-purpose compression methods are less effective because the label data consists of large low-frequency regions with structured boundaries unlike natural image data. We present Compresso, a new compression scheme for label data that outperforms existing approaches by using a sliding window to exploit redundancy across border regions in 2D and 3D. We compare our method to existing compression schemes and provide a detailed evaluation on eleven biomedical and image segmentation datasets. Our method provides a factor of 600-2200x compression for label volumes, with running times suitable for practice.},
  booktitle={Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  pages={781--788},
  year={2017},
  organization={Springer, Cham},
  code={https://github.com/VCG/compresso},
  poster={http://mpsych.org/papers/matejek2017compresso_poster.pdf},
  shortvenue={MICCAI 2017}
}

@inproceedings{gonda2017icon,
  title={ICON: An Interactive Approach to train Deep Neural Networks for Segmentation of Neuronal Structures},
  author={Gonda, Felix and Kaynig, Verena and Jones, Thouis R. and \myname{Haehn, Daniel} and Lichtman, Jeff W. and Parag, Toufiq and Pfister, Hanspeter},
  abstract={We present an interactive approach to train a deep neural network pixel classifier for the segmentation of neuronal structures. An interactive training scheme reduces the extremely tedious manual annotation task that is typically required for deep networks to perform well on image segmentation problems. Our proposed method employs a feedback loop that captures sparse annotations using a graphical user interface, trains a deep neural network based on recent and past annotations, and displays the prediction output to users in almost real-time. Our implementation of the algorithm also allows multiple users to provide annotations in parallel and receive feedback from the same classifier. Quick feedback on classifier performance in an interactive setting enables users to identify and label examples that are more important than others for segmentation purposes. Our experiments show that an interactively-trained pixel classifier produces better region segmentation results on Electron Microscopy (EM) images than those generated by a network of the same architecture trained offline on exhaustive ground-truth labels.},
  booktitle={IEEE International Symposium on Biomedical Imaging (ISBI)},
  pages={327--331},
  year={2017},
  organization={IEEE},
  code={https://github.com/Rhoana/icon},
  shortvenue={ISBI 2017}
}

@inproceedings{pienaar2017chips,
  title={CHIPS--A Service for Collecting, Organizing, Processing, and Sharing Medical Image Data in the Cloud},
  author={Pienaar, Rudolph and Turk, Ata and Bernal-Rusiel, Jorge and Rannou, Nicolas and \myname{Haehn, Daniel} and Grant, P. Ellen and Krieger, Orran},
  abstract={Web browsers are increasingly used as middleware platforms offering a central access point for service provision. Using backend containerization, RESTful APIs, and distributed computing allows for complex systems to be realized that address the needs of modern compute intense environments. In this paper, we present a web-based medical image data and information management software platform called CHIPS (Cloud Healthcare Image Processing Service). This cloud-based services allows for authenticated and secure retrieval of medical image data from resources typically found in hospitals, organizes and presents information in a modern feed-like interface, provides access to a growing library of plugins that process these data, allows for easy data sharing between users and provides powerful 3D visualization and real-time collaboration. Image processing is orchestrated across additional cloud-based resources using containerization technologies.},
  booktitle={VLDB Workshop on Data Management and Analytics for Medicine and Healthcare},
  pages={29--35},
  year={2017},
  organization={Springer, Cham},
  code={https://github.com/FNNDSC/ChRIS_ultron_backEnd},
  shortvenue={VLDB Workshop 2017}
}

%
% 2016
%
@article{suissa2016automatic,
  title={Automatic Neural Reconstruction from Petavoxel of Electron Microscopy Data},
  author={Suissa-Peleg, Adi and \myname{Haehn, Daniel} and Knowles-Barley, Seymour and Kaynig, Verena and Jones, Thouis R. and Wilson, Alyssa and Schalek, Richard and Lichtman, Jeff W. and Pfister, Hanspeter},
  abstract={Connectomics is the study of the dense structure of the neurons in the brain and their synapses, providing new insights into the relation between braintextquoterights structure and its function. Recent advances in Electron Microscopy enable high-resolution imaging (4nm per pixel) of neural tissue at a rate of roughly 10 terapixels in a single day, allowing neuroscientists to capture large blocks of neural tissue in a reasonable amount of time. The large amounts of data require novel computer vision based algorithms and scalable software frameworks to process this data. We describe RhoANA, our dense Automatic Neural Annotation framework, which we have developed in order to automatically align, segment and reconstruct a 1mm^3 brain tissue (~2 peta-pixels).},
  journal={Microscopy and Microanalysis},
  volume={22},
  pages={536},
  year={2016},
  publisher={Cambridge University Press},
  shortvenue={Microscopy and Microanalysis 2016}
}

@article{alawami2016neuroblocks,
  title={NeuroBlocks--Visual Tracking of Segmentation and Proofreading for Large Connectomics Projects},
  author={Al-Awami, Ali K. and Beyer, Johanna and \myname{Haehn, Daniel} and Kasthuri, Narayanan and Lichtman, Jeff W. and Pfister, Hanspeter and Hadwiger, Markus},
  abstract={In the field of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the first system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specific tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.},
  journal={IEEE Transactions on Visualization and Computer Graphics (IEEE VIS)},
  volume={22},
  number={1},
  pages={738--746},
  year={2016},
  publisher={IEEE},
  video={https://www.youtube.com/watch?v=uuiYvntM0Ik},
  website={http://people.seas.harvard.edu/~jbeyer/neuroblocks.html},
  shortvenue={IEEE VIS 2016}
}

@article{schalek2016imaging,
  title={Imaging a 1 mm$^3$ Volume of Rat Cortex using a MultiBeam SEM},
  author={Schalek, Richard and Lee, Dong and Kasthuri, Narayanan and Peleg, Adi and Jones, Thouis R. and Kaynig, Verena and \myname{Haehn, Daniel} and Pfister, Hanspeter and Cox, David and Lichtman, Jeff W.},
  abstract={The rodent brain is organized with length scales spanning centimeters to nanometers &mdash;6 orders of magnitude. At the centimeter scale, the brain consist of lobes of cortex, the cerebellum, the brainstem and the spinal cord. The millimeter scale have neurons arranged in columns, layers, or otherwise clustered. Recent technological imaging advances allow the generation of neuronal datasets spanning the spatial range from nanometers to 100s of microns. Collecting a 1 mm^3 volume dataset of brain tissue at 4 nm x-y resolution using the fastest signal-beam SEM would require ~6 years. To move to the next length and volume scale of neuronal circuits requires several technological advances. The multibeam scanning electron microscope (mSEM) represents a transformative imaging technology that enables neuroscientists to tackle millimeter scale cortical circuit problems. In this work we describe a workflow from tissue harvest to imaging that will generate a 2 petabyte dataset (> 300,000,000 images) of rat visual cortex imaged at a 4nm x 4nm x-y (Nyquist sampling of membranes) and 30nm section thickness in less than 6 months.},
  journal={Microscopy and Microanalysis},
  volume={22},
  pages={582},
  year={2016},
  publisher={Cambridge University Press},
  shortvenue={Microscopy and Microanalysis 2016}
}

%
% 2015
%
@article{im2015altered,
  title={Altered Structural Brain Networks in Tuberous Sclerosis Complex},
  author={Im, Kiho and Ahtam, Banu and \myname{Haehn, Daniel} and Peters, Jurriaan M. and Warfield, Simon K. and Sahin, Mustafa and Grant, P. Ellen},
  abstract={Tuberous sclerosis complex (TSC) is characterized by benign hamartomas in multiple organs including the brain and its clinical phenotypes may be associated with abnormal neural connections. We aimed to provide the first detailed findings on disrupted structural brain networks in TSC patients. Structural whole-brain connectivity maps were constructed using structural and diffusion MRI in 20 TSC (age range: 3–24 years) and 20 typically developing (TD; 3–23 years) subjects. We assessed global (short and long-association and interhemispheric fibers) and regional white matter connectivity, and performed graph theoretical analysis using gyral pattern- and atlas-based node parcellations. Significantly higher mean diffusivity (MD) was shown in TSC patients than in TD controls throughout the whole brain and positively correlated with tuber load severity. A significant increase in MD was mainly influenced by an increase in radial diffusivity. Furthermore, interhemispheric connectivity was particularly reduced in TSC, which leads to increased network segregation within hemispheres. TSC patients with developmental delay (DD) showed significantly higher MD than those without DD primarily in intrahemispheric connections. Our analysis allows nonbiased determination of differential white matter involvement, which may provide better measures of "lesion load" and lead to a better understanding of disease mechanisms.},
  journal={Cerebral Cortex},
  volume={26},
  number={5},
  pages={2046--2058},
  year={2015},
  publisher={Oxford University Press},
  supplemental={https://academic.oup.com/cercor/article/26/5/2046/1754153#supplementary-data},
  shortvenue={Cerebral Cortex 2015}
}

@inproceedings{pienaar2015chris,
  title={ChRIS--A web-based Neuroimaging and Informatics System for Collecting, Organizing, Processing, Visualizing and Sharing of Medical Data},
  author={Pienaar, Rudolph and Rannou, Nicolas and Bernal, Jorge and \myname{Haehn, Daniel} and Grant, P. Ellen},
  abstract={The utility of web browsers for general purpose computing, long anticipated, is only now coming into fruition. In this paper we present a web-based medical image data and information management software platform called ChRIS ([Boston] Children’s Research Integration System). ChRIS’ deep functionality allows for easy retrieval of medical image data from resources typically found in hospitals, organizes and presents information in a modern feed-like interface, provides access to a growing library of plugins that process these data – typically on a connected High Performance Compute Cluster, allows for easy data sharing between users and instances of ChRIS and provides powerful 3D visualization and real time collaboration.},
  booktitle={IEEE Engineering in Medicine and Biology Society (EMBC)},
  pages={206--209},
  year={2015},
  organization={IEEE},
  code={https://github.com/FNNDSC/ChRIS_ultron_backEnd},
  shortvenue={EMBC 2015}
}

%
% 2014
%
@article{haehn2014design,
  title={Design and Evaluation of Interactive Proofreading Tools for Connectomics},
  author={\myname{Haehn, Daniel} and Knowles-Barley, Seymour and Roberts, Mike and Beyer, Johanna and Kasthuri, Narayanan and Lichtman, Jeff W. and Pfister, Hanspeter},
  abstract={Proofreading refers to the manual correction of automatic segmentations of image data. In connectomics, electron microscopy data is acquired at nanometer-scale resolution and results in very large image volumes of brain tissue that require fully automatic segmentation algorithms to identify cell boundaries. However, these algorithms require hundreds of corrections per cubic micron of tissue. Even though this task is time consuming, it is fairly easy for humans to perform corrections through splitting, merging, and adjusting segments during proofreading. In this paper we present the design and implementation of Mojo, a fully-featured single-user desktop application for proofreading, and Dojo, a multi-user web-based application for collaborative proofreading. We evaluate the accuracy and speed of Mojo, Dojo, and Raveler, a proofreading tool from Janelia Farm, through a quantitative user study. We designed a between-subjects experiment and asked non-experts to proofread neurons in a publicly available connectomics dataset. Our results show a significant improvement of corrections using web-based Dojo even in comparison to fully manual expert segmentation, when given the same amount of time. In addition, all participants using Dojo reported better usability. We discuss our findings and provide an analysis of requirements for designing visual proofreading software.},
  journal={IEEE Transactions on Visualization and Computer Graphics (IEEE VIS)},
  volume={20},
  number={12},
  pages={2466--2475},
  year={2014},
  publisher={IEEE},
  website={http://rhoana.org/dojo/},
  code={http://github.com/rhoana/dojo/},
  data={https://github.com/haehn/proofreading},
  video={https://vimeo.com/102949056},
  shortvenue={IEEE VIS 2014}
}

%
% 2013
%
@inproceedings{haehn2013slice,
  title={Slice:Drop -- Collaborative Medical Imaging in the Browser},
  author={\myname{Haehn, Daniel} and Rannou, Nicolas and Grant, P. Ellen and Pienaar, Rudolph},
  abstract={This project demonstrates real-time rendering and sharing of standard medical data formats between WebGL-enabled browsers across multiple devices. Any linked browser can interact with and update the display, which propogates to all other linked browsers. We think that the best way to render your files is without any necessary conversions. Just drop them on the Slice:Drop website and they are ready to render. Slice:Drop supports a variety of scientific file formats out-of-the-box and uses WebGL and HTML5 Canvas to render the data in 2D and 3D.},
  booktitle={ACM SIGGRAPH Computer Animation Festival},
  year={2013},
  organization={ACM},
  website={http://slicedrop.com},
  code={http://github.com/slicedrop/slicedrop.github.com/},
  poster={http://mpsych.org/papers/haehn2013slice_poster.pdf},
  video={https://vimeo.com/280534894},
  shortvenue={SIGGRAPH Real-Time Live! 2013}
}

%
% 2012
%
@ARTICLE{haehn2012neuroimaging,
  AUTHOR={\myname{Haehn, Daniel}  and  Rannou, Nicolas  and  Ahtam, Banu  and  Grant, P. Ellen  and  Pienaar, Rudolph},   
  TITLE={Neuroimaging in the Browser using the X Toolkit},
  ABSTRACT={WebGL is recent technology that exposes a computer's GPU to a browser, and allows for the native generation of rich three dimensional graphics. Compatible web browsers can offer a graphical experience comparable to more traditional stand-alone programs. As such WebGL offers the potential of bringing the "web" to neuroscience, and has great potential to accelerate and support scientific research. Cognisant of these possibilities, we present 'The X Toolkit' (XTK), the first JavaScript-based framework for visualizing and interacting with medical imaging data using WebGL.},      
  JOURNAL={Frontiers in Neuroinformatics},      
  YEAR={2012},
  POSTER={http://mpsych.org/papers/haehn2012neuroimaging_poster.pdf},
  WEBSITE={http://goXTK.com},
  CODE={http://github.com/xtk/X},
  spotlight={yes},
  POSTER={http://mpsych.org/papers/haehn2012neuroimaging_poster.pdf},
  shortvenue={Neuroinformatics 2012}
}

@article{choe2012regional,
  title={Regional Infant Brain Development: an MRI-based Morphometric Analysis in 3 to 13 month olds},
  author={Choe, Myong-sun and Ortiz-Mantilla, Silvia and Makris, Nikos and Gregas, Matt and Bacic, Janine and \myname{Haehn, Daniel} and Kennedy, David and Pienaar, Rudolph and Caviness Jr, Verne S. and Benasich, April A. and Grant, P. Ellen},
  abstract={Elucidation of infant brain development is a critically important goal given the enduring impact of these early processes on various domains including later cognition and language. Although infants' whole-brain growth rates have long been available, regional growth rates have not been reported systematically. Accordingly, relatively less is known about the dynamics and organization of typically developing infant brains. Here we report global and regional volumetric growth of cerebrum, cerebellum, and brainstem with gender dimorphism, in 33 cross-sectional scans, over 3 to 13 months, using T1-weighted 3-dimensional spoiled gradient echo images and detailed semi-automated brain segmentation. Except for the midbrain and lateral ventricles, all absolute volumes of brain regions showed significant growth, with 6 different patterns of volumetric change. When normalized to the whole brain, the regional increase was characterized by 5 differential patterns. The putamen, cerebellar hemispheres, and total cerebellum were the only regions that showed positive growth in the normalized brain. Our results show region-specific patterns of volumetric change and contribute to the systematic understanding of infant brain development. This study greatly expands our knowledge of normal development and in future may provide a basis for identifying early deviation above and beyond normative variation that might signal higher risk for neurological disorders.},
  journal={Cerebral Cortex},
  volume={23},
  number={9},
  pages={2100--2117},
  year={2012},
  publisher={Oxford University Press},
  supplemental={https://academic.oup.com/cercor/article/23/9/2100/595802?searchresult=1#supplementary-data},
  shortvenue={Cerebral Cortex 2012}
}

@ARTICLE{klein2012mindboggle,
  title={Mindboggle: Automated Human Brain MRI Feature Extraction, Labeling, Morphometry, and Online Visualization},
  author={Klein, Arno and Bao, Forrest S. and H{\"a}me, Yrj{\"o} and Stavsky, Eliezer and Giard, Joachim and \myname{Haehn, Daniel} and Nichols, Nolan and Ghosh, Satrajit S.},
  abstract={Mindboggle is a new neuroinformatics platform that currently automates the extraction, labeling, and morphometry of cortical features derived from human brain MR imaging data. It will soon be released as open source Python (and Python-wrapped C++) software built within nipype's flexible and modular software pipeline framework.},
  journal={Frontiers in Neuroinformatics},
  year={2012},
  WEBSITE={http://mindboggle.info/},
  CODE={http://mindboggle.readthedocs.io/en/latest/},
  DATA={http://mindboggle.info/data.html},
  shortvenue={Neuroinformatics 2012}
}

@ARTICLE{klein2012mindboggle2,
  AUTHOR={Klein, Arno  and  Nichols, Nolan  and  \myname{Haehn, Daniel}},   
  TITLE={Mindboggle 2 interface: Online Visualization of Extracted Brain Features with XTK},   
  ABSTRACT={The Mindboggle project automates anatomical brain labeling, feature extraction and identification, and shape analysis of brain regions and features. This generates a lot of data, which presents a challenge for visualization and comparison across brains. In the past, it would have been untenable to present rich, three-dimensional data interactively and online within a web browser, but recent developments in WebGL libraries have made it possible. XTK is the first WebGL library geared towards 3D medical imaging data. We demonstrate a web interface to the Mindboggle data, database, and software with interactive visualizations of manually and automatically labeled brain regions and hierarchical features using XTK. },   
  JOURNAL={Frontiers in Neuroinformatics},
  YEAR={2012},
  WEBSITE={http://mindboggle.info/},
  CODE={http://mindboggle.readthedocs.io/en/latest/},
  DATA={http://mindboggle.info/data.html},
  shortvenue={Neuroinformatics 2012}
}






