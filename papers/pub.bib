%
% 2020
%
@inproceedings{franke2020modern,
  title={Modern Scientific Visualizations on the Web},
  author={Franke, Loraine and Haehn, Daniel},
  booktitle={MDPI Informatics},
  volume={7},
  number={4},
  pages={37},
  year={2020},
  abstract={Modern scientific visualization is web-based and uses emerging technology such as WebGL (Web Graphics Library) and WebGPU for three-dimensional computer graphics and WebXR for augmented and virtual reality devices. These technologies, paired with the accessibility of websites, potentially offer a user experience beyond traditional standalone visualization systems. We review the state-of-the-art of web-based scientific visualization and present an overview of existing methods categorized by application domain. As part of this analysis, we introduce the Scientific Visualization Future Readiness Score (SciVis FRS) to rank visualizations for a technology-driven disruptive tomorrow. We then summarize challenges, current state of the publication trend, future directions, and opportunities for this exciting research field.},
  organization={Multidisciplinary Digital Publishing Institute},
  code={https://github.com/mpsych/SciVis-Web},
  data={https://github.com/mpsych/SciVis-Web},
  website={https://github.com/mpsych/SciVis-Web},
  shortvenue={MDPI Informatics 2020}
}

@inproceedings{haehn2020trako,
  title={TRAKO: Efficient Transmission of Tractography Data for Visualization},
  author={\myname{Haehn, Daniel} and Franke, Loraine and Zhang, Fan and Karayumak, Suheyla Cetin and Pieper, Steve and O'Donnell, Lauren and Rathi, Yogesh},
  abstract={Fiber tracking produces large tractography datasets that are tens of gigabytes in size consisting of millions of streamlines. Such vast amounts of data require formats that allow for efficient storage, transfer, and visualization. We present TRAKO, a new data format based on the Graphics Layer Transmission Format (glTF) that enables immediate graphical and hardware-accelerated processing. We integrate a state-of-the-art compression technique for vertices, streamlines, and attached scalar and property data. We then compare TRAKO to existing tractography storage methods and provide a detailed evaluation on eight datasets. TRAKO can achieve data reductions of over 28x without loss of statistical significance when used to replicate analysis from previously published studies. },
  booktitle={Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  pages={XXX--XXX},
  year={2020},
  supplemental={http://danielhaehn.com/papers/haehn2020trako_supplemental.pdf},
  organization={Springer, Cham},
  code={https://github.com/bostongfx/TRAKO/},
  data={https://github.com/bostongfx/TRAKO/},
  website={https://pypi.org/project/trako/},
  shortvenue={MICCAI 2020}
}

@article{lin2020twostream,
  title={Two Stream Active Query Suggestion for Active Learning in Connectomics},
  author={Lin, Zudi and Wei, Donglai and Jang, Won-Dong and Zhou, Siyan and Chen, Xupeng and Wang, Xueying and Schalek, Richard and Berger, Daniel and Matejek, Brian and Kamentsky, Lee and Peleg, Adi and \myname{Haehn, Daniel} and Jones, Thouis R. and Parag, Toufiq and Lichtman, Jeff and Pfister, Hanspeter},
  abstract={For large-scale vision tasks in biomedical images, the labeled data is often limited to train effective deep models. Active learning is a common solution, where a query suggestion method selects representative unlabeled samples for annotation, and the new labels are used to improve the base model. However, most query suggestion models optimize their learnable parameters only on the limited labeled data and consequently become less effective for the more challenging unlabeled data. To tackle this, we propose a two-stream active query suggestion approach. In addition to the supervised feature extractor, we introduce an unsupervised one optimized on all raw images to capture diverse image features, which can later be improved by fine-tuning on new labels. As a use case, we build an end-to-end active learning framework with our query suggestion method for 3D synapse detection and mitochondria segmentation in connectomics. With the framework, we curate, to our best knowledge, the largest connectomics dataset with dense synapses and mitochondria annotation.},
  booktitle = {European Conference on Computer Vision (ECCV)},
  month = {August},
  year = {2020},
  supplemental={http://danielhaehn.com/papers/lin2020twostream_supplemental.pdf},
  code={https://github.com/zudi-lin/pytorch_connectomics/},
  data={https://zudi-lin.github.io/projects/},
  website={https://zudi-lin.github.io/projects/#two_stream_active},
  shortvenue={ECCV 2020}
}

@article {lekschas2020peax,
    title={Peax: Interactive Visual Pattern Search in Sequential Data Using Unsupervised Deep Representation Learning},
    author={Lekschas, Fritz and Peterson, Brant and Haehn, Daniel and Ma, Eric and Gehlenborg, Nils and Pfister, Hanspeter},
    abstract={We present Peax, a novel feature-based technique for interactive visual pattern search in sequential data, like time series or data mapped to a genome sequence. Visually searching for patterns by similarity is often challenging because of the large search space, the visual complexity of patterns, and the user’s perception of similarity. For example, in genomics, researchers try to link patterns in multivariate sequential data to cellular or pathogenic processes, but a lack of ground truth and high variance makes automatic pattern detection unreliable. We have developed a convolutional autoencoder for unsupervised representation learning of regions in sequential data that can capture more visual details of complex patterns compared to existing similarity measures. Using this learned representation as features of the sequential data, our accompanying visual query system enables interactive feedback-driven adjustments of the pattern search to adapt to the users’ perceived similarity. Using an active learning sampling strategy, Peax collects user-generated binary relevance feedback. This feedback is used to train a model for binary classification, to ultimately find other regions that exhibit patterns similar to the search target. We demonstrate Peax’s features through a case study in genomics and report on a user study with eight domain experts to assess the usability and usefulness of Peax. Moreover, we evaluate the effectiveness of the learned feature representation for visual similarity search in two additional user studies. We find that our models retrieve significantly more similar patterns than other commonly used techniques.},
    journal={Computer Graphics Forum},
    publisher={The Eurographics Association and John Wiley & Sons Ltd.},
    year={2020},
    bestpaper={yes},
    issn={1467-8659},
    doi={10.1111/cgf.13971},
    supplemental={https://mpsych.org/papers/lekschas2020peax_supplemental.pdf},
    code={https://github.com/flekschas/peax-experiment},
    data={https://github.com/flekschas/peax-experiment},
    website={http://peax.lekschas.de/},
    shortvenue={EuroVis 2020 (Best Paper Award)}
}

@inproceedings{casser2020fast,
  title={Fast Mitochondria Detection for Connectomics},
  author={Casser, Vincent and Kang, Kai and Pfister, Hanspeter and \myname{Daniel Haehn}},
  abstract={High-resolution connectomics data allows for the identification of dysfunctional mitochondria which are linked to a variety of diseases such as autism or bipolar. However, manual analysis is not feasible since datasets can be petabytes in size. We present a fully automatic mitochondria detector based on a modified U-Net architecture that yields high accuracy and fast processing times. We evaluate our method on multiple real-world connectomics datasets, including an improved version of the EPFL mitochondria benchmark. Our results show an Jaccard index of up to 0.90 with inference times lower than 16ms for a 512x512px image tile. This speed is faster than the acquisition speed of modern electron microscopes, enabling mitochondria detection in real-time. Our detector ranks first for real-time detection when compared to previous works and data, results, and code are openly available.},
  booktitle={International Conference on Medical Imaging with Deep Learning},
  pages={XXX--XXX},
  year={2020},
  spotlight={yes},
  website={https://sites.google.com/view/connectomics/},
  code={https://github.com/mpsych/mitochondria/},
  data={https://sites.google.com/view/connectomics/},
  shortvenue={MIDL 2020 (Spotlight)}
}


%
% 2019
%
@InProceedings{matejek2019biologically,
    title={Biologically-Constrained Graphs for Global Connectomics Reconstruction},
    author={Matejek, Brian and \myname{Haehn, Daniel} and Zhu, Haidong and Wei, Donglai and Parag, Toufiq and Pfister, Hanspeter},
    abstract={Most current state-of-the-art connectome reconstruction pipelines have two major steps: initial pixel-based segmentation with affinity prediction and watershed transform, and refined segmentation by merging over-segmented regions. These methods rely only on local context and are typically agnostic to the underlying biology. Since a few merge errors can lead to several incorrectly merged neuronal processes, these algorithms are currently tuned towards over-segmentation producing an overburden of costly proofreading. We propose a third step for connectomics reconstruction pipelines to refine an over-segmentation using both local and global context with an emphasis on adhering to the underlying biology. We first extract a graph from an input segmentation where nodes correspond to segment labels and edges indicate potential split errors in the over-segmentation. In order to increase throughput and allow for large-scale reconstruction, we employ biologically inspired geometric constraints based on neuron morphology to reduce the number of nodes and edges. Next, two neural networks learn these neuronal shapes to further aid the graph construction process. Lastly, we reformulate the region merging problem as a graph partitioning one to leverage global context. We demonstrate the performance of our approach on four real-world connectomics datasets with an average variation of information improvement of 21.3%.},
    booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019},
    code={https://github.com/bmatejek/ibex/},
    shortvenue={CVPR 2019}
}

%
% 2018
%
@article{haehn2018evaluating,
  title={Evaluating 'Graphical Perception' with CNNs},
  author={\myname{Haehn, Daniel} and Tompkin, James and Pfister, Hanspeter},
  journal={IEEE Transactions on Visualization and Computer Graphics (IEEE VIS)},
  abstract={Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill’s seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.},
  volume={25},
  pages={641--650},
  number={1},
  year={2018},
  month={October},
  publisher={IEEE},
  supplemental={http://mpsych.org/papers/haehn2018evaluating_supplemental.pdf},
  code={http://rhoana.org/perception/},
  data={http://rhoana.org/perception/},
  video={https://vimeo.com/280506639},
  fastforward={https://vimeo.com/285106317},
  poster={http://mpsych.org/papers/haehn2018evaluating_poster.pdf},
  shortvenue={IEEE VIS 2018}
}

@InProceedings{haehn2018guided,
    title={Guided Proofreading of Automatic Segmentations for Connectomics},
    author={\myname{Haehn, Daniel} and Kaynig, Verena and Tompkin, James and Lichtman, Jeff W. and Pfister, Hanspeter},
    abstract={Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.},
    booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2018},
    supplemental={http://mpsych.org/papers/haehn2018guided_supplemental.pdf},
    code={http://rhoana.org/guidedproofreading/},
    data={https://github.com/haehn/proofreading},
    video={https://vimeo.com/280507933},
    poster={http://mpsych.org/papers/haehn2018guided_poster.pdf},
    shortvenue={CVPR 2018}
}

%
% 2017
%
@article{haehn2017scalable,
  title={Scalable Interactive Visualization for Connectomics},
  author={\myname{Haehn, Daniel} and Hoffer, John and Matejek, Brian and Suissa-Peleg, Adi and Al-Awami, Ali K. and Kamentsky, Lee and Gonda, Felix and Meng, Eagon and Zhang, William and Schalek, Richard and Wilson, Alyssa and Parag, Toufiq and Beyer, Johanna and Kaynig, Verena and Jones, Thouis R. and Tompkin, James and Hadwiger, Markus and Lichtman, Jeff W. and Pfister, Hanspeter},
  journal={MDPI Informatics},
  abstract={Connectomics has recently begun to image brain tissue at nanometer resolution, which produces petabytes of data. This data must be aligned, labeled, proofread, and formed into graphs, and each step of this process requires visualization for human verification. As such, we present the BUTTERFLY middleware, a scalable platform that can handle massive data for interactive visualization in connectomics. Our platform outputs image and geometry data suitable for hardware-accelerated rendering, and abstracts low-level data wrangling to enable faster development of new visualizations. We demonstrate scalability and extendability with a series of open source Web-based applications for every step of the typical connectomics workflow: data management and storage, informative queries, 2D and 3D visualizations, interactive editing, and graph-based analysis. We report design choices for all developed applications and describe typical scenarios of isolated and combined use in everyday connectomics research. In addition, we measure and optimize rendering throughput—from storage to display—in quantitative experiments. Finally, we share insights, experiences, and recommendations for creating an open source data management and interactive visualization platform for connectomics.},
  volume={4},
  number={3},
  pages={29},
  year={2017},
  organization={Multidisciplinary Digital Publishing Institute},
  code={https://github.com/Rhoana/butterfly},
  video={https://vimeo.com/280509756},
  shortvenue={MDPI Informatics 2017}
}

@inproceedings{matejek2017compresso,
  title={Compresso: Efficient Compression of Segmentation Data For Connectomics},
  author={Matejek, Brian and \myname{Haehn, Daniel} and Lekschas, Fritz and Mitzenmacher, Michael and Pfister, Hanspeter},
  abstract={Recent advances in segmentation methods for connectomics and biomedical imaging produce very large datasets with labels that assign object classes to image pixels. The resulting label volumes are bigger than the raw image data and need compression for efficient storage and transfer. General-purpose compression methods are less effective because the label data consists of large low-frequency regions with structured boundaries unlike natural image data. We present Compresso, a new compression scheme for label data that outperforms existing approaches by using a sliding window to exploit redundancy across border regions in 2D and 3D. We compare our method to existing compression schemes and provide a detailed evaluation on eleven biomedical and image segmentation datasets. Our method provides a factor of 600-2200x compression for label volumes, with running times suitable for practice.},
  booktitle={Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  pages={781--788},
  year={2017},
  organization={Springer, Cham},
  code={https://github.com/VCG/compresso},
  poster={http://mpsych.org/papers/matejek2017compresso_poster.pdf},
  shortvenue={MICCAI 2017}
}

@inproceedings{gonda2017icon,
  title={ICON: An Interactive Approach to train Deep Neural Networks for Segmentation of Neuronal Structures},
  author={Gonda, Felix and Kaynig, Verena and Jones, Thouis R. and \myname{Haehn, Daniel} and Lichtman, Jeff W. and Parag, Toufiq and Pfister, Hanspeter},
  abstract={We present an interactive approach to train a deep neural network pixel classifier for the segmentation of neuronal structures. An interactive training scheme reduces the extremely tedious manual annotation task that is typically required for deep networks to perform well on image segmentation problems. Our proposed method employs a feedback loop that captures sparse annotations using a graphical user interface, trains a deep neural network based on recent and past annotations, and displays the prediction output to users in almost real-time. Our implementation of the algorithm also allows multiple users to provide annotations in parallel and receive feedback from the same classifier. Quick feedback on classifier performance in an interactive setting enables users to identify and label examples that are more important than others for segmentation purposes. Our experiments show that an interactively-trained pixel classifier produces better region segmentation results on Electron Microscopy (EM) images than those generated by a network of the same architecture trained offline on exhaustive ground-truth labels.},
  booktitle={IEEE International Symposium on Biomedical Imaging (ISBI)},
  pages={327--331},
  year={2017},
  organization={IEEE},
  code={https://github.com/Rhoana/icon},
  shortvenue={ISBI 2017}
}

@inproceedings{pienaar2017chips,
  title={CHIPS--A Service for Collecting, Organizing, Processing, and Sharing Medical Image Data in the Cloud},
  author={Pienaar, Rudolph and Turk, Ata and Bernal-Rusiel, Jorge and Rannou, Nicolas and \myname{Haehn, Daniel} and Grant, P. Ellen and Krieger, Orran},
  abstract={Web browsers are increasingly used as middleware platforms offering a central access point for service provision. Using backend containerization, RESTful APIs, and distributed computing allows for complex systems to be realized that address the needs of modern compute intense environments. In this paper, we present a web-based medical image data and information management software platform called CHIPS (Cloud Healthcare Image Processing Service). This cloud-based services allows for authenticated and secure retrieval of medical image data from resources typically found in hospitals, organizes and presents information in a modern feed-like interface, provides access to a growing library of plugins that process these data, allows for easy data sharing between users and provides powerful 3D visualization and real-time collaboration. Image processing is orchestrated across additional cloud-based resources using containerization technologies.},
  booktitle={VLDB Workshop on Data Management and Analytics for Medicine and Healthcare},
  pages={29--35},
  year={2017},
  organization={Springer, Cham},
  code={https://github.com/FNNDSC/ChRIS_ultron_backEnd},
  shortvenue={VLDB Workshop 2017}
}

%
% 2016
%
@article{suissa2016automatic,
  title={Automatic Neural Reconstruction from Petavoxel of Electron Microscopy Data},
  author={Suissa-Peleg, Adi and \myname{Haehn, Daniel} and Knowles-Barley, Seymour and Kaynig, Verena and Jones, Thouis R. and Wilson, Alyssa and Schalek, Richard and Lichtman, Jeff W. and Pfister, Hanspeter},
  abstract={Connectomics is the study of the dense structure of the neurons in the brain and their synapses, providing new insights into the relation between braintextquoterights structure and its function. Recent advances in Electron Microscopy enable high-resolution imaging (4nm per pixel) of neural tissue at a rate of roughly 10 terapixels in a single day, allowing neuroscientists to capture large blocks of neural tissue in a reasonable amount of time. The large amounts of data require novel computer vision based algorithms and scalable software frameworks to process this data. We describe RhoANA, our dense Automatic Neural Annotation framework, which we have developed in order to automatically align, segment and reconstruct a 1mm^3 brain tissue (~2 peta-pixels).},
  journal={Microscopy and Microanalysis},
  volume={22},
  pages={536},
  year={2016},
  publisher={Cambridge University Press},
  shortvenue={Microscopy and Microanalysis 2016}
}

@article{alawami2016neuroblocks,
  title={NeuroBlocks--Visual Tracking of Segmentation and Proofreading for Large Connectomics Projects},
  author={Al-Awami, Ali K. and Beyer, Johanna and \myname{Haehn, Daniel} and Kasthuri, Narayanan and Lichtman, Jeff W. and Pfister, Hanspeter and Hadwiger, Markus},
  abstract={In the field of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the first system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specific tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.},
  journal={IEEE Transactions on Visualization and Computer Graphics (IEEE VIS)},
  volume={22},
  number={1},
  pages={738--746},
  year={2016},
  publisher={IEEE},
  video={https://www.youtube.com/watch?v=uuiYvntM0Ik},
  website={http://people.seas.harvard.edu/~jbeyer/neuroblocks.html},
  shortvenue={IEEE VIS 2016}
}

@article{schalek2016imaging,
  title={Imaging a 1 mm$^3$ Volume of Rat Cortex using a MultiBeam SEM},
  author={Schalek, Richard and Lee, Dong and Kasthuri, Narayanan and Peleg, Adi and Jones, Thouis R. and Kaynig, Verena and \myname{Haehn, Daniel} and Pfister, Hanspeter and Cox, David and Lichtman, Jeff W.},
  abstract={The rodent brain is organized with length scales spanning centimeters to nanometers &mdash;6 orders of magnitude. At the centimeter scale, the brain consist of lobes of cortex, the cerebellum, the brainstem and the spinal cord. The millimeter scale have neurons arranged in columns, layers, or otherwise clustered. Recent technological imaging advances allow the generation of neuronal datasets spanning the spatial range from nanometers to 100s of microns. Collecting a 1 mm^3 volume dataset of brain tissue at 4 nm x-y resolution using the fastest signal-beam SEM would require ~6 years. To move to the next length and volume scale of neuronal circuits requires several technological advances. The multibeam scanning electron microscope (mSEM) represents a transformative imaging technology that enables neuroscientists to tackle millimeter scale cortical circuit problems. In this work we describe a workflow from tissue harvest to imaging that will generate a 2 petabyte dataset (> 300,000,000 images) of rat visual cortex imaged at a 4nm x 4nm x-y (Nyquist sampling of membranes) and 30nm section thickness in less than 6 months.},
  journal={Microscopy and Microanalysis},
  volume={22},
  pages={582},
  year={2016},
  publisher={Cambridge University Press},
  shortvenue={Microscopy and Microanalysis 2016}
}

%
% 2015
%
@article{im2015altered,
  title={Altered Structural Brain Networks in Tuberous Sclerosis Complex},
  author={Im, Kiho and Ahtam, Banu and \myname{Haehn, Daniel} and Peters, Jurriaan M. and Warfield, Simon K. and Sahin, Mustafa and Grant, P. Ellen},
  abstract={Tuberous sclerosis complex (TSC) is characterized by benign hamartomas in multiple organs including the brain and its clinical phenotypes may be associated with abnormal neural connections. We aimed to provide the first detailed findings on disrupted structural brain networks in TSC patients. Structural whole-brain connectivity maps were constructed using structural and diffusion MRI in 20 TSC (age range: 3–24 years) and 20 typically developing (TD; 3–23 years) subjects. We assessed global (short and long-association and interhemispheric fibers) and regional white matter connectivity, and performed graph theoretical analysis using gyral pattern- and atlas-based node parcellations. Significantly higher mean diffusivity (MD) was shown in TSC patients than in TD controls throughout the whole brain and positively correlated with tuber load severity. A significant increase in MD was mainly influenced by an increase in radial diffusivity. Furthermore, interhemispheric connectivity was particularly reduced in TSC, which leads to increased network segregation within hemispheres. TSC patients with developmental delay (DD) showed significantly higher MD than those without DD primarily in intrahemispheric connections. Our analysis allows nonbiased determination of differential white matter involvement, which may provide better measures of "lesion load" and lead to a better understanding of disease mechanisms.},
  journal={Cerebral Cortex},
  volume={26},
  number={5},
  pages={2046--2058},
  year={2015},
  publisher={Oxford University Press},
  supplemental={https://academic.oup.com/cercor/article/26/5/2046/1754153#supplementary-data},
  shortvenue={Cerebral Cortex 2015}
}

@inproceedings{pienaar2015chris,
  title={ChRIS--A web-based Neuroimaging and Informatics System for Collecting, Organizing, Processing, Visualizing and Sharing of Medical Data},
  author={Pienaar, Rudolph and Rannou, Nicolas and Bernal, Jorge and \myname{Haehn, Daniel} and Grant, P. Ellen},
  abstract={The utility of web browsers for general purpose computing, long anticipated, is only now coming into fruition. In this paper we present a web-based medical image data and information management software platform called ChRIS ([Boston] Children’s Research Integration System). ChRIS’ deep functionality allows for easy retrieval of medical image data from resources typically found in hospitals, organizes and presents information in a modern feed-like interface, provides access to a growing library of plugins that process these data – typically on a connected High Performance Compute Cluster, allows for easy data sharing between users and instances of ChRIS and provides powerful 3D visualization and real time collaboration.},
  booktitle={IEEE Engineering in Medicine and Biology Society (EMBC)},
  pages={206--209},
  year={2015},
  organization={IEEE},
  code={https://github.com/FNNDSC/ChRIS_ultron_backEnd},
  shortvenue={EMBC 2015}
}

%
% 2014
%
@article{haehn2014design,
  title={Design and Evaluation of Interactive Proofreading Tools for Connectomics},
  author={\myname{Haehn, Daniel} and Knowles-Barley, Seymour and Roberts, Mike and Beyer, Johanna and Kasthuri, Narayanan and Lichtman, Jeff W. and Pfister, Hanspeter},
  abstract={Proofreading refers to the manual correction of automatic segmentations of image data. In connectomics, electron microscopy data is acquired at nanometer-scale resolution and results in very large image volumes of brain tissue that require fully automatic segmentation algorithms to identify cell boundaries. However, these algorithms require hundreds of corrections per cubic micron of tissue. Even though this task is time consuming, it is fairly easy for humans to perform corrections through splitting, merging, and adjusting segments during proofreading. In this paper we present the design and implementation of Mojo, a fully-featured single-user desktop application for proofreading, and Dojo, a multi-user web-based application for collaborative proofreading. We evaluate the accuracy and speed of Mojo, Dojo, and Raveler, a proofreading tool from Janelia Farm, through a quantitative user study. We designed a between-subjects experiment and asked non-experts to proofread neurons in a publicly available connectomics dataset. Our results show a significant improvement of corrections using web-based Dojo even in comparison to fully manual expert segmentation, when given the same amount of time. In addition, all participants using Dojo reported better usability. We discuss our findings and provide an analysis of requirements for designing visual proofreading software.},
  journal={IEEE Transactions on Visualization and Computer Graphics (IEEE VIS)},
  volume={20},
  number={12},
  pages={2466--2475},
  year={2014},
  publisher={IEEE},
  website={http://rhoana.org/dojo/},
  code={http://github.com/rhoana/dojo/},
  data={https://github.com/haehn/proofreading},
  video={https://vimeo.com/102949056},
  shortvenue={IEEE VIS 2014}
}

%
% 2013
%
@inproceedings{haehn2013slice,
  title={Slice:Drop -- Collaborative Medical Imaging in the Browser},
  author={\myname{Haehn, Daniel} and Rannou, Nicolas and Grant, P. Ellen and Pienaar, Rudolph},
  abstract={This project demonstrates real-time rendering and sharing of standard medical data formats between WebGL-enabled browsers across multiple devices. Any linked browser can interact with and update the display, which propogates to all other linked browsers. We think that the best way to render your files is without any necessary conversions. Just drop them on the Slice:Drop website and they are ready to render. Slice:Drop supports a variety of scientific file formats out-of-the-box and uses WebGL and HTML5 Canvas to render the data in 2D and 3D.},
  booktitle={ACM SIGGRAPH Computer Animation Festival},
  year={2013},
  organization={ACM},
  website={http://slicedrop.com},
  code={http://github.com/slicedrop/slicedrop.github.com/},
  poster={http://mpsych.org/papers/haehn2013slice_poster.pdf},
  video={https://vimeo.com/280534894},
  shortvenue={SIGGRAPH Real-Time Live! 2013}
}

%
% 2012
%
@ARTICLE{haehn2012neuroimaging,
  AUTHOR={\myname{Haehn, Daniel}  and  Rannou, Nicolas  and  Ahtam, Banu  and  Grant, P. Ellen  and  Pienaar, Rudolph},   
  TITLE={Neuroimaging in the Browser using the X Toolkit},
  ABSTRACT={WebGL is recent technology that exposes a computer's GPU to a browser, and allows for the native generation of rich three dimensional graphics. Compatible web browsers can offer a graphical experience comparable to more traditional stand-alone programs. As such WebGL offers the potential of bringing the "web" to neuroscience, and has great potential to accelerate and support scientific research. Cognisant of these possibilities, we present 'The X Toolkit' (XTK), the first JavaScript-based framework for visualizing and interacting with medical imaging data using WebGL.},      
  JOURNAL={Frontiers in Neuroinformatics},      
  YEAR={2012},
  POSTER={http://mpsych.org/papers/haehn2012neuroimaging_poster.pdf},
  WEBSITE={http://goXTK.com},
  CODE={http://github.com/xtk/X},
  spotlight={yes},
  POSTER={http://mpsych.org/papers/haehn2012neuroimaging_poster.pdf},
  shortvenue={Neuroinformatics 2012}
}

@article{choe2012regional,
  title={Regional Infant Brain Development: an MRI-based Morphometric Analysis in 3 to 13 month olds},
  author={Choe, Myong-sun and Ortiz-Mantilla, Silvia and Makris, Nikos and Gregas, Matt and Bacic, Janine and \myname{Haehn, Daniel} and Kennedy, David and Pienaar, Rudolph and Caviness Jr, Verne S. and Benasich, April A. and Grant, P. Ellen},
  abstract={Elucidation of infant brain development is a critically important goal given the enduring impact of these early processes on various domains including later cognition and language. Although infants' whole-brain growth rates have long been available, regional growth rates have not been reported systematically. Accordingly, relatively less is known about the dynamics and organization of typically developing infant brains. Here we report global and regional volumetric growth of cerebrum, cerebellum, and brainstem with gender dimorphism, in 33 cross-sectional scans, over 3 to 13 months, using T1-weighted 3-dimensional spoiled gradient echo images and detailed semi-automated brain segmentation. Except for the midbrain and lateral ventricles, all absolute volumes of brain regions showed significant growth, with 6 different patterns of volumetric change. When normalized to the whole brain, the regional increase was characterized by 5 differential patterns. The putamen, cerebellar hemispheres, and total cerebellum were the only regions that showed positive growth in the normalized brain. Our results show region-specific patterns of volumetric change and contribute to the systematic understanding of infant brain development. This study greatly expands our knowledge of normal development and in future may provide a basis for identifying early deviation above and beyond normative variation that might signal higher risk for neurological disorders.},
  journal={Cerebral Cortex},
  volume={23},
  number={9},
  pages={2100--2117},
  year={2012},
  publisher={Oxford University Press},
  supplemental={https://academic.oup.com/cercor/article/23/9/2100/595802?searchresult=1#supplementary-data},
  shortvenue={Cerebral Cortex 2012}
}

@ARTICLE{klein2012mindboggle,
  title={Mindboggle: Automated Human Brain MRI Feature Extraction, Labeling, Morphometry, and Online Visualization},
  author={Klein, Arno and Bao, Forrest S. and H{\"a}me, Yrj{\"o} and Stavsky, Eliezer and Giard, Joachim and \myname{Haehn, Daniel} and Nichols, Nolan and Ghosh, Satrajit S.},
  abstract={Mindboggle is a new neuroinformatics platform that currently automates the extraction, labeling, and morphometry of cortical features derived from human brain MR imaging data. It will soon be released as open source Python (and Python-wrapped C++) software built within nipype's flexible and modular software pipeline framework.},
  journal={Frontiers in Neuroinformatics},
  year={2012},
  WEBSITE={http://mindboggle.info/},
  CODE={http://mindboggle.readthedocs.io/en/latest/},
  DATA={http://mindboggle.info/data.html},
  shortvenue={Neuroinformatics 2012}
}

@ARTICLE{klein2012mindboggle2,
  AUTHOR={Klein, Arno  and  Nichols, Nolan  and  \myname{Haehn, Daniel}},   
  TITLE={Mindboggle 2 interface: Online Visualization of Extracted Brain Features with XTK},   
  ABSTRACT={The Mindboggle project automates anatomical brain labeling, feature extraction and identification, and shape analysis of brain regions and features. This generates a lot of data, which presents a challenge for visualization and comparison across brains. In the past, it would have been untenable to present rich, three-dimensional data interactively and online within a web browser, but recent developments in WebGL libraries have made it possible. XTK is the first WebGL library geared towards 3D medical imaging data. We demonstrate a web interface to the Mindboggle data, database, and software with interactive visualizations of manually and automatically labeled brain regions and hierarchical features using XTK. },   
  JOURNAL={Frontiers in Neuroinformatics},
  YEAR={2012},
  WEBSITE={http://mindboggle.info/},
  CODE={http://mindboggle.readthedocs.io/en/latest/},
  DATA={http://mindboggle.info/data.html},
  shortvenue={Neuroinformatics 2012}
}






